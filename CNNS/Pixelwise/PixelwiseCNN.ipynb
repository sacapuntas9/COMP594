{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "trainDir = \"E:\\\\594_data\\\\custom_NN_Data\\\\train\"\n",
    "validDir = \"E:\\\\594_data\\\\custom_NN_Data\\\\valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, \"int32\")\n",
    "    y_pred = tf.cast(tf.round(y_pred), \"int32\") # implicit 0.5 threshold via tf.round\n",
    "    y_correct = y_true * y_pred\n",
    "    sum_true = tf.reduce_sum(y_true, axis=1)\n",
    "    sum_pred = tf.reduce_sum(y_pred, axis=1)\n",
    "    sum_correct = tf.reduce_sum(y_correct, axis=1)\n",
    "    precision = sum_correct / sum_pred\n",
    "    recall = sum_correct / sum_true\n",
    "    f_score = 5 * precision * recall / (4 * precision + recall)\n",
    "    f_score = tf.where(tf.is_nan(f_score), tf.zeros_like(f_score), f_score)\n",
    "    return tf.reduce_mean(f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 195159 images belonging to 2 classes.\n",
      "Found 26418 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        data_format='channels_first',\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,         \n",
    "                                  data_format='channels_first'\n",
    "                                 )\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        trainDir,  # this is the target directory\n",
    "        target_size=(65, 65),  # all images will be resized\n",
    "        color_mode='grayscale',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validDir,\n",
    "        target_size=(65, 65),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5624/5625 [============================>.] - ETA: 0s - loss: 0.0625 - binary_accuracy: 0.9837\n",
      "Epoch 00001: val_binary_accuracy improved from -inf to 0.98656, saving model to weights.balanced.hdf5\n",
      "5625/5625 [==============================] - 2710s 482ms/step - loss: 0.0625 - binary_accuracy: 0.9837 - val_loss: 0.0511 - val_binary_accuracy: 0.9866\n",
      "Epoch 2/10\n",
      "5621/5625 [============================>.] - ETA: 0s - loss: 0.0586 - binary_accuracy: 0.9855\n",
      "Epoch 00002: val_binary_accuracy did not improve\n",
      "5625/5625 [==============================] - 193s 34ms/step - loss: 0.0586 - binary_accuracy: 0.9855 - val_loss: 0.0671 - val_binary_accuracy: 0.9865\n",
      "Epoch 3/10\n",
      "5624/5625 [============================>.] - ETA: 0s - loss: 0.0598 - binary_accuracy: 0.9853\n",
      "Epoch 00003: val_binary_accuracy improved from 0.98656 to 0.98747, saving model to weights.balanced.hdf5\n",
      "5625/5625 [==============================] - 76s 14ms/step - loss: 0.0598 - binary_accuracy: 0.9853 - val_loss: 0.0519 - val_binary_accuracy: 0.9875\n",
      "Epoch 4/10\n",
      "5624/5625 [============================>.] - ETA: 0s - loss: 0.0622 - binary_accuracy: 0.9855\n",
      "Epoch 00004: val_binary_accuracy did not improve\n",
      "5625/5625 [==============================] - 73s 13ms/step - loss: 0.0622 - binary_accuracy: 0.9855 - val_loss: 0.0554 - val_binary_accuracy: 0.9862\n",
      "Epoch 5/10\n",
      "5624/5625 [============================>.] - ETA: 0s - loss: 0.0639 - binary_accuracy: 0.9850- ETA: 2s\n",
      "Epoch 00005: val_binary_accuracy did not improve\n",
      "5625/5625 [==============================] - 74s 13ms/step - loss: 0.0639 - binary_accuracy: 0.9850 - val_loss: 0.1128 - val_binary_accuracy: 0.9804\n",
      "Epoch 6/10\n",
      "5622/5625 [============================>.] - ETA: 0s - loss: 0.0689 - binary_accuracy: 0.9842\n",
      "Epoch 00006: val_binary_accuracy improved from 0.98747 to 0.98759, saving model to weights.balanced.hdf5\n",
      "5625/5625 [==============================] - 73s 13ms/step - loss: 0.0688 - binary_accuracy: 0.9842 - val_loss: 0.0645 - val_binary_accuracy: 0.9876\n",
      "Epoch 7/10\n",
      "5624/5625 [============================>.] - ETA: 0s - loss: 0.0661 - binary_accuracy: 0.9850\n",
      "Epoch 00007: val_binary_accuracy did not improve\n",
      "5625/5625 [==============================] - 73s 13ms/step - loss: 0.0661 - binary_accuracy: 0.9850 - val_loss: 0.0540 - val_binary_accuracy: 0.9865\n",
      "Epoch 8/10\n",
      "5620/5625 [============================>.] - ETA: 0s - loss: 0.0680 - binary_accuracy: 0.9847- ETA: 0s - loss: 0.0678 - binary_ac\n",
      "Epoch 00008: val_binary_accuracy did not improve\n",
      "5625/5625 [==============================] - 76s 13ms/step - loss: 0.0680 - binary_accuracy: 0.9847 - val_loss: 0.1216 - val_binary_accuracy: 0.9738\n",
      "Epoch 9/10\n",
      "5622/5625 [============================>.] - ETA: 0s - loss: 0.0728 - binary_accuracy: 0.9839\n",
      "Epoch 00009: val_binary_accuracy did not improve\n",
      "5625/5625 [==============================] - 76s 13ms/step - loss: 0.0728 - binary_accuracy: 0.9839 - val_loss: 0.0673 - val_binary_accuracy: 0.9863\n",
      "Epoch 10/10\n",
      "5620/5625 [============================>.] - ETA: 0s - loss: 0.0695 - binary_accuracy: 0.9843\n",
      "Epoch 00010: val_binary_accuracy did not improve\n",
      "5625/5625 [==============================] - 77s 14ms/step - loss: 0.0694 - binary_accuracy: 0.9843 - val_loss: 0.0517 - val_binary_accuracy: 0.9864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b87f6edac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(1,65,65 ), data_format='channels_first'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "#model.add(Activation('softmax'))\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.add(Activation('tanh'))\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"weights.balanced.hdf5\", verbose=1, monitor='val_binary_accuracy',\n",
    "                             save_best_only=True,\n",
    "                             mode='auto')\n",
    "\n",
    "model.compile(loss='binary_crossentropy',  \n",
    "              optimizer='rmsprop',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= 180000 // batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps= 25000 // batch_size,\n",
    "        verbose=1, \n",
    "        callbacks=[checkpoint,tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
